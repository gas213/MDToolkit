#!/bin/sh

# From MDToolkit template file: sol.sh

# === README ===
# This script is designed to be submitted from inside a user-created, run-specific directory in a "home-like" space (backed up, or intended to be backed up). The directory should only contain:
#   - This script
#   - The LAMMPS input file for the run
#   - Any data/restart file(s) required for the run to begin - make sure these are backed up somewhere else! They will be moved into the scratch-like space, not copied.
# The script will create an identical directory structure at the SCRATCH_DIR location, which is where dump/restart files will be generated.
# Any .txt files generated by LAMMPS via fix commands should be automatically copied back to the home-like space after the run (unless the job terminates for some other reason).
# However, the LAMMPS log file will be generated directly on the home-like space.
# If the simulation runs to completion, the only action that should be required is to back up any dump/restart files that you want from the SCRATCH_DIR space.



# vvv REPLACE THE {{FIELDS}} vvv

# Partitions info:
# rapids: max 3 days, 64 CPUs per node
# hawkcpu: max 3 days, 50 CPUs per node
# rapids-express: max 2 hours, max 6 CPUs
# hawkcpu-express: max 6 hours, max 6 CPUs

#SBATCH --partition={{rapids, hawkcpu, rapids-express, hawkcpu-express}}
#SBATCH --time={{d-hh:mm:ss}}
#SBATCH --nodes={{n}}
#SBATCH --ntasks-per-node={{64, 50, 6}}
#SBATCH --job-name={{name}}
#SBATCH --output="job.%j.%N.out"
#SBATCH --mail-type=ALL
#SBATCH --mail-user={{email}}

# The name of our group's ceph allocation directory
ALLOCATION=ebw210_093025

# User's scratch-like directory where dumps and restarts will be generated; must end with a forward slash /
SCRATCH_DIR=$HOME/$ALLOCATION/$USER/_scratchlike/

# Directory where the user's spack installation is located (the same directory where md_install_here.sh was executed from)
SPACK_DIR=$HOME/$ALLOCATION/$USER/md_env/



# ====================================
# THE REST OF THIS SHOULD BE AUTOMATIC
# ====================================

RUN_DIR_HOME="$PWD/"
RUN_DIR_RELATIVE="${PWD##$HOME/$ALLOCATION/$USER/}/"
RUN_DIR_SCRATCH="$SCRATCH_DIR$RUN_DIR_RELATIVE"

# Prevent overwriting an existing run by checking if there is already a directory in the scratch-like space with the same name
if [ -d $RUN_DIR_SCRATCH ]; then
    echo "ERROR Scratch-like run directory already exists: $RUN_DIR_SCRATCH"
    exit 1
# Make sure there is exactly one LAMMPS input file for this run
elif [ $(find $RUN_DIR_HOME -maxdepth 1 -name "*.in" | wc -l) -ne 1 ]; then
    echo "ERROR There must be exactly one .in file in the home run directory."
    exit 1
fi

IN_PATH_HOME=$(find $RUN_DIR_HOME -maxdepth 1 -name "*.in")
IN_PATH_RELATIVE=${IN_PATH_HOME##$RUN_DIR_HOME}

# Create identical run directory structure on the scratch space and copy LAMMPS input file there
mkdir -p $RUN_DIR_SCRATCH
cp $IN_PATH_RELATIVE $RUN_DIR_SCRATCH

# Move any data/restart files to the scratch destination; first copy them and then ensure the copy is identical before deleting the original
shopt -s nullglob
for data_restart_file in *.data *.restart *.rs; do
    cp $data_restart_file $RUN_DIR_SCRATCH
    if cmp -s $data_restart_file "$RUN_DIR_SCRATCH$data_restart_file"; then
        rm $data_restart_file
    else
        echo "ERROR failed to copy $data_restart_file into scratch space, or the file was copied but is not identical to the original."
        exit 1
    fi
done

# Activate spack environment
cd $SPACK_DIR
. spack/share/spack/setup-env.sh
spack env activate md

# Switch to the new run directory on the scratch-like space and start the run
# Write LAMMPS log to the home-like space
cd $RUN_DIR_SCRATCH
mpirun -np $SLURM_NTASKS `which lmp` -in $IN_PATH_RELATIVE -log "${RUN_DIR_HOME}log.lammps"

# After the run, copy any txt files generated by LAMMPS back to the home-like space
if [ $(find $RUN_DIR_SCRATCH -maxdepth 1 -name "*.txt" | wc -l) -gt 0 ]; then
    cp *.txt $RUN_DIR_HOME
fi

exit